package lexar

import (
	"errors"
	"fmt"
	"reflect"
	"testing"

	"github.com/codecrafters-io/interpreter-starter-go/app/pkg/token"
)

type TestCase struct {
	input  string
	output []token.Token
	errors []error
}

func TestLexar(t *testing.T) {
	cases := []TestCase{
		{
			input: "(()",
			output: []token.Token{
				{TokenType: token.LeftParen, Lexeme: "(", Literal: nil},
				{TokenType: token.LeftParen, Lexeme: "(", Literal: nil},
				{TokenType: token.RightParen, Lexeme: ")", Literal: nil},
				{TokenType: token.Eof, Lexeme: "", Literal: nil},
			},
		},
		{
			input: "{{}}",
			output: []token.Token{
				{TokenType: token.LeftBrace, Lexeme: "{", Literal: nil},
				{TokenType: token.LeftBrace, Lexeme: "{", Literal: nil},
				{TokenType: token.RightBrace, Lexeme: "}", Literal: nil},
				{TokenType: token.RightBrace, Lexeme: "}", Literal: nil},
				{TokenType: token.Eof, Lexeme: "", Literal: nil},
			},
		},
		{
			input: "({*.,+*-;})",
			output: []token.Token{
				{TokenType: token.LeftParen, Lexeme: "(", Literal: nil},
				{TokenType: token.LeftBrace, Lexeme: "{", Literal: nil},
				{TokenType: token.Star, Lexeme: "*", Literal: nil},
				{TokenType: token.Dot, Lexeme: ".", Literal: nil},
				{TokenType: token.Comma, Lexeme: ",", Literal: nil},
				{TokenType: token.Plus, Lexeme: "+", Literal: nil},
				{TokenType: token.Star, Lexeme: "*", Literal: nil},
				{TokenType: token.Minus, Lexeme: "-", Literal: nil},
				{TokenType: token.Semicolon, Lexeme: ";", Literal: nil},
				{TokenType: token.RightBrace, Lexeme: "}", Literal: nil},
				{TokenType: token.RightParen, Lexeme: ")", Literal: nil},
				{TokenType: token.Eof, Lexeme: "", Literal: nil},
			},
		},
		{
			input: "{{$}}#",
			output: []token.Token{
				{TokenType: token.LeftBrace, Lexeme: "{", Literal: nil},
				{TokenType: token.LeftBrace, Lexeme: "{", Literal: nil},
				{TokenType: token.RightBrace, Lexeme: "}", Literal: nil},
				{TokenType: token.RightBrace, Lexeme: "}", Literal: nil},
				{TokenType: token.Eof, Lexeme: "", Literal: nil},
			},
			errors: []error{errors.New("[line 1] Error: Unexpected character: $"), errors.New("[line 1] Error: Unexpected character: #")},
		},
		{
			input: "==,=,!!==,>>=<<=",
			output: []token.Token{
				{TokenType: token.EqualEqual, Lexeme: "==", Literal: nil},
				{TokenType: token.Comma, Lexeme: ",", Literal: nil},
				{TokenType: token.Equal, Lexeme: "=", Literal: nil},
				{TokenType: token.Comma, Lexeme: ",", Literal: nil},
				{TokenType: token.Bang, Lexeme: "!", Literal: nil},
				{TokenType: token.BangEqual, Lexeme: "!=", Literal: nil},
				{TokenType: token.Equal, Lexeme: "=", Literal: nil},
				{TokenType: token.Comma, Lexeme: ",", Literal: nil},
				{TokenType: token.Greater, Lexeme: ">", Literal: nil},
				{TokenType: token.GreaterEqual, Lexeme: ">=", Literal: nil},
				{TokenType: token.Less, Lexeme: "<", Literal: nil},
				{TokenType: token.LessEqual, Lexeme: "<=", Literal: nil},
				{TokenType: token.Eof, Lexeme: "", Literal: nil},
			},
		},
		{
			input: "/,\n//coMment\n,\t \r",
			output: []token.Token{
				{TokenType: token.Slash, Lexeme: "/", Literal: nil},
				{TokenType: token.Comma, Lexeme: ",", Literal: nil},
				{TokenType: token.Comma, Lexeme: ",", Literal: nil},
				{TokenType: token.Eof, Lexeme: "", Literal: nil},
			},
		},
		{
			input: "/\",,123\"/",
			output: []token.Token{
				{TokenType: token.Slash, Lexeme: "/", Literal: nil},
				{TokenType: token.StringToken, Lexeme: "\",,123\"", Literal: ",,123"},
				{TokenType: token.Slash, Lexeme: "/", Literal: nil},
				{TokenType: token.Eof, Lexeme: "", Literal: nil},
			},
		},
		{
			input: "\"baz\" . \"unterminated",
			output: []token.Token{
				{TokenType: token.StringToken, Lexeme: "\"baz\"", Literal: "baz"},
				{TokenType: token.Dot, Lexeme: ".", Literal: nil},
				{TokenType: token.Eof, Lexeme: "", Literal: nil},
			},
			errors: []error{errors.New("[line 1] Error: Unterminated string.")},
		},
		{
			input: "/123.123/134/123.00000",
			output: []token.Token{
				{TokenType: token.Slash, Lexeme: "/", Literal: nil},
				{TokenType: token.NumberFloat, Lexeme: "123.123", Literal: 123.123},
				{TokenType: token.Slash, Lexeme: "/", Literal: nil},
				{TokenType: token.NumberInt, Lexeme: "134", Literal: int64(134)},
				{TokenType: token.Slash, Lexeme: "/", Literal: nil},
				{TokenType: token.NumberFloat, Lexeme: "123.00000", Literal: 123.0},
				{TokenType: token.Eof, Lexeme: "", Literal: nil},
			},
		},
		{
			input: "/foo bar foo_123/",
			output: []token.Token{
				{TokenType: token.Slash, Lexeme: "/", Literal: nil},
				{TokenType: token.Identifier, Lexeme: "foo", Literal: nil},
				{TokenType: token.Identifier, Lexeme: "bar", Literal: nil},
				{TokenType: token.Identifier, Lexeme: "foo_123", Literal: nil},
				{TokenType: token.Slash, Lexeme: "/", Literal: nil},
				{TokenType: token.Eof, Lexeme: "", Literal: nil},
			},
		},
		{
			input: "/foo while foo_123/ nil",
			output: []token.Token{
				{TokenType: token.Slash, Lexeme: "/", Literal: nil},
				{TokenType: token.Identifier, Lexeme: "foo", Literal: nil},
				{TokenType: token.WhileToken, Lexeme: "while", Literal: nil},
				{TokenType: token.Identifier, Lexeme: "foo_123", Literal: nil},
				{TokenType: token.Slash, Lexeme: "/", Literal: nil},
				{TokenType: token.NilToken, Lexeme: "nil", Literal: nil},
				{TokenType: token.Eof, Lexeme: "", Literal: nil},
			},
		},
	}

	for _, testCase := range cases {
		t.Run(fmt.Sprintf("running input: %v", testCase.input), func(t *testing.T) {
			lexar := NewLexar([]byte(testCase.input))
			tokens, errors := lexar.Scan()

			if len(testCase.output) != len(tokens) {
				t.Errorf("length mismatch: expected %d tokens, got %d",
					len(testCase.output), len(tokens))
			}

			if len(testCase.errors) != len(errors) {
				t.Errorf("length mismatch: expected %d errors, got %d",
					len(testCase.errors), len(errors))
			}

			for i := range testCase.errors {
				expected := testCase.errors[i]
				got := errors[i]

				if expected.Error() != got.Error() {
					t.Errorf("\nerror[%d] mismatch:\n expected: %+q\n got:      %+q", i, expected, got)
				}
			}

			for i := range testCase.output {
				expected := testCase.output[i]
				got := tokens[i]

				if expected != got {
					t.Errorf("\ntoken[%d] mismatch:\n  expected: %+v\n got:      %+v", i, expected, got)
					t.Errorf("\n token lexeme type expected: %v, got: %v", reflect.TypeOf(expected.Lexeme), reflect.TypeOf(got.Lexeme))
					t.Errorf("\n token literal type expected: %v, got: %v", reflect.TypeOf(expected.Literal), reflect.TypeOf(got.Literal))
				}
			}
		})
	}
}
